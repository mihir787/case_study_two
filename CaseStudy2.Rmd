---
title: "Case Study 2"
author: Mihir Parikh, Lokesh Maganti, Aaron Tomkins
output:
  html_document:
    keep_md: true
---

```{r libraries, include=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(MASS)
library(readxl)
library(randomForest)
```

<center><h1>Introduction</h1></center>

DDSAnalytics is an analytics company that specializes in talent management solutions for Fortune 1000 companies. Talent management is defined as the iterative process of developing and retaining employees. It may include workforce planning, employee training programs, identifying high-potential employees and reducing/preventing voluntary employee turnover (attrition). To gain a competitive edge over its competition, DDSAnalytics decided to leverage data science for talent management. The executive leadership identified predicting employee turnover as its first application of data science for talent management. Before the business green lights the project, they tasked the data science team to conduct an analysis of existing employee data. The scope of this report is to summarize those findings.

To conduct exploratory data analysis (EDA), the data science team was provided with CaseStudy2Data.zip file to determine factors that lead to attrition. From this data, the team was asked to identify (at least) the top three factors that contribute to turnover, to learn about any job role specific trends that may exist in the data set (e.g., "Data Scientists have the highest job satisfaction") and to provide any other interesting trends and observations. All the Experiments and analysis were conducted in R.

<center><h1>Context on Process</h1></center>

The analysis and modeling was done both in R and Python which interms provides interesting insights as to how language specific packages yield potentially different different models. To view the executed python code please look at case_study_two.md. The python code here is not executed but the output is provided.

<center><h1>Pre-Process Data</h1></center>

### In R

The main steps we took in cleaning the data was to remove the columns that behaved like constants (i.e., they had the same value for all observations), and transform columns into factors that were incorrectly imported as character data types.

We also split our data into training and test sets (using a 80/20 split) in order to test the accuracy of various models that were utilized.
```{r, echo=TRUE}
# --- Import data.
df <- read_xlsx("CaseStudy2data.xlsx", col_names = TRUE)

# --- Transform data.
# Transform character types to factors.
df <- cbind(df[,-c(2,3,5,8,12,16,18,22,23)],lapply(df[,c(2,3,5,8,12,16,18,22,23)], as.factor))
# Create X matrix by removing the response, employee number, over 18 (factor with 1 level). We also removed
# employee count and standard hours for having constant values.
X.df <- df[,-c(5,6,18,27,34)]
# Create y vector.
y.df <- as.factor(df$Attrition)
# Combine these two for functions that require the full data frame. Then rename the response column.
df.new <- cbind(X.df, y.df)
names(df.new)[names(df.new) == 'y.df'] <- 'Attrition'

# --- Split data into training and test sets.
set.seed(5)
train <- sample(1:dim(X.df)[1], dim(X.df)[1]*0.8)
test <- -train
```

### In Python

```{pCleanup eval=FALSE}
file = 'CaseStudy2data.xlsx'

# Load spreadsheet
xl = pd.ExcelFile(file)

# Load a sheet into a DataFrame by name: df1
unchanged_attrition_df = xl.parse('HR-employee-attrition Data')
attrition_df = unchanged_attrition_df

# drop unneccessary columns with constant values
attrition_column = attrition_df["Attrition"]
attrition_df.drop("Attrition", axis=1, inplace=True)
attrition_df.insert(0, "Attrition", attrition_column)
attrition_df.drop('EmployeeCount', axis=1, inplace=True)
attrition_df.drop('StandardHours', axis=1, inplace=True)
attrition_df.drop('Over18', axis=1, inplace=True)

# convert attrition variable to binary
attrition_df = attrition_df.replace("Yes", 1)
attrition_df = attrition_df.replace("No", 0)

# code dummy variables, this is necessary to use sklearn
attrition_df = pd.get_dummies(attrition_df, columns=["BusinessTravel", "Department", "EducationField", "Gender", "JobRole", "MaritalStatus"], prefix=["BusinessTravel", "Department", "EduationField", "Gender", "JobRole", "MaritalStatus"])
df_without_attrition = attrition_df.drop("Attrition", axis=1)
columns_without_attrition = df_without_attrition.columns
columns_with_attrition = attrition_df.columns

# split test and train set
rand = np.random.rand(len(attrition_df)) < 0.8

train = attrition_df[rand]
test = attrition_df[~rand]
```

<center><h1>Exploratory Data Analysis</h1></center>
*Conducted in Python*

```{pHeatMap eval=FALSE}
# provide squares for correlation heatplot for values above +/- .6
plt.figure(figsize = (20,20))
corr = attrition_df.corr()
sns.heatmap(corr[(corr >= 0.6) | (corr <= -0.6)], 
        xticklabels=columns_with_attrition,
        yticklabels=columns_with_attrition, linewidths=.6)
```
![](case_study_two_files/case_study_two_6_1.png)

Based off looking at the correlation heatmap above, there are not any values that stand out as extremly correlated (0.8). Thus no variables will be removed.

```{pattritionCounts eval=FALSE}
sns.countplot(x='Attrition',data=attrition_df)
plt.show()
```
![](case_study_two_files/case_study_two_8_0.png)

This is a look at the number of people who left their job vs the number of people who stayed. 



```{pjobRoleCounts eval=FALSE}
sns.countplot(y="JobRole", data=unchanged_attrition_df)
plt.show()
```
![](case_study_two_files/case_study_two_9_0.png)

This is a look at the number of people in each Job Role.

```{pJobRoleAttritionCounts eval=FALSE}
sns.countplot(y="JobRole", hue="Attrition", data=unchanged_attrition_df)
plt.show()
```
![](case_study_two_files/case_study_two_22_1.png)

A look at the ratio of attrition for each Job Role.

```{pMonthlyIncomeByRoleCounts eval=FALSE}
income = unchanged_attrition_df.groupby('JobRole').mean()[['MonthlyIncome']].values
flattened_income = []
for i in income:
    flattened_income.append(i[0])
    
roles = ['Healthcare Representative', 'Human Resources', 'Labrator Technician', 'Manager', 'Manufacturing Director', 'Research Director', 'Research Scientist' , 'Sales Executive','Sales Representative']

y_pos = np.arange(len(roles))

plt.xlabel('Average Monthly Income')
plt.ylabel('Job Role')
 
# Create horizontal bars
plt.barh(y_pos, flattened_income)
 
# Create names on the y-axis
plt.yticks(y_pos, roles)
 
# Show graphic
plt.show()
```
![](case_study_two_files/case_study_two_10_0.png)

This is a look at the Average Monthly Income of respective Job Roles.

<center><h1>Modeling - Determine Most Influential Factors</h1></center>

### In R

#### Random Forrest Model

The four most important factors in predicting employee attrition per our random forest model are:
1. MonthlyIncome
2. Age
3. Overtime
4. DailyRate

Our random forest model also had 100% accuracy on the test set, which supports the fact that these four explanatory variables can definitely contribute to accurately predicting employee attrition (for comparison, the naive method of always predicting the majority class would lead to accuracy of 84.69%).
```{r, echo=TRUE}
# *** Random Forest ***
attr.rfor <- randomForest(X.df, y.df, mtry = ceiling(sqrt(dim(X.df)[2])), subset = train)
importance(attr.rfor)

# Calculate accuracy. Accuracy is 100%.
rfor.predict <- predict(attr.rfor, df.new[test,])
sum(rfor.predict == df.new$Attrition[test])/length(df.new$Attrition[test])

```

#### Logistic Regression Model

Since random forest models do not provide p-values or coefficients, we tried fitting a logistic regression model to provide a little more interpretability. However, this model identified different variables as being important than our random forest model. The six most important variables (that were also statistically significant) from the logistic regression model were:
1. OverTime
2. BusinessTravel_Frequently
3. JobInvolvement
4. EnvironmentSatisfaction
5. JobSatsifaction
6. RelationshipSatisfaction.

The reason the variables that were identified as important through logistic regression were different from the random forest is due to different assumptions about the structure of the data when using logistic regression. The logistic regression model we applied assumes linearity, constant variance, independence, and also had no interaction terms.

In order to see which model more accurately represents the data, we calculate the accuracy of our logistic regression model on our test set and got 88.44% accuracy. Since this is lower than our random forest model, we would put more weight on the variables identified from our random forest model and conclude that some of the regression assumptions were violated. We can create a more advanced logistic regression model that corrects for some of these issues in the future if deemed necessary.
```{r, echo=TRUE}
# *** Logistic Regression ***

attr.logi <- glm(Attrition ~ ., family = binomial, data = df.new, subset = train)
summary(attr.logi)

# Data frame of the most important variables.
Variable_Name <- c('OverTime','BusinessTravel_Freq','JobInvolvement','EnvironSatisfaction','JobSatisfaction','RelationshipSatisfaction')
Variable_Coef <- c(1.86, 1.70, abs(-0.678), abs(-0.521), abs(-0.383), abs(-0.320))
logi.df <- data.frame(cbind(Variable_Name, Variable_Coef))
logi.df <- transform(logi.df, Variable_Name = reorder(Variable_Name, order(Variable_Coef, decreasing = FALSE)))

# Plot the important variables.
ggplot(data = logi.df) + geom_bar(mapping = aes(x = Variable_Name, y = Variable_Coef, fill = Variable_Name), stat = "identity") + ggtitle("Most Important Variables from Logistic Regression") + theme(plot.title = element_text(hjust = 0.5)) + labs(x = "Variable Name", y = "Coefficient (Absolute Value)") + coord_flip()

# Calculate accuracy.
logi.pred <- predict(attr.logi, df.new[test,], type = "response")
logi.newpred <- rep("No", length(df.new$Attrition[test]))
logi.newpred[logi.pred > 0.60] = "Yes"
# Accuracy is 88.44% (the naive method has accuracy of 84.69%).
sum(logi.newpred == df.new$Attrition[test])/length(df.new$Attrition[test])

```

#### Linear Discriminant Analysis Model (LDA)
Since our logistic regression model did not identify the same significant variables as the random forest model, we instead turned to a linear discriminant analysis model in order to look at how the variables identified in the random forest model differ between employees that left and those that stayed within the company (the probability of the X, given Y). Per LDA (and Naive Bayes), if the mean of an explanatory variable is very different between the response categories, then that explanatory variable can be useful for predicting the response. A table of the different means of the variables identified from our random forest does show that they differ considerably between employees that left and those that stayed (see table below).

Since LDA assumes that the explanatory variables are normally-distributed, however, we do not believe it will be accurate in its predictions (since there are many categorical variables in our model, and variables that are dummy encoded in a binary way are not Gaussian). And as expected, our LDA model had accuracy of 87.07% of our test set (lower than the logistic regression model).
```{r, echo=TRUE}
# *** Linear Discriminant Analysis ***
attr.lda <- lda(Attrition ~ ., data = df.new, subset = train)
attr.lda$means

# Data frame of the important variables from the random forest.
val.lda <- c('MonthlyIncome','Age','OverTime','DailyRate')
noavg.lda <- c(6869.48, 37.65, 0.24, 811.29)
yesavg.lda <- c(4720.71, 33.58, 0.52, 750.63)
lda.df <- data.frame(cbind(val.lda, noavg.lda, yesavg.lda))
names(lda.df) <- c('Variable','Avg Stay','Avg Leave')
lda.df

# Calculate accuracy.
lda.pred <- predict(attr.lda, df.new[test,])
# Accuracy is 87.07% (the naive method has accuracy of 84.69%).
sum(lda.pred$class == df.new$Attrition[test])/length(df.new$Attrition[test])
```

### In Python
*Note that only 2 of many attempted models are displayed here. For more insights on some of the other models considered please look at case_study_two.md*

#### Extra Tree Classifier Model

This modeling technique is similar to Random Forrest except for the fact that random splits are tested as opposed to all possible splits. Extratrees are hence cheaper to train and can sometimes end up generalizing better.

```{pExtraTreeClassifier eval=FALSE}
from sklearn.ensemble import ExtraTreesClassifier
# feature extraction
model = ExtraTreesClassifier()
fit = model.fit(X, Y)

importances = model.feature_importances_
print(importances)

idxs = np.argsort(importances)[-(number_of_features_to_select):]
features = np.asarray(columns_without_attrition[idxs])
print(list(reversed(features.tolist())))

predictions = model.predict(X_test)
print('Accuracy of extra trees classifier on test set: {:.5f}'.format(model.score(X_test, Y_test)))
```

![](screenshots/extraTree.png)

In order of most important features:
1. OverTime |
2. Age |
3. MontlyIncome |
4. JobSatisfaction |
5. EnvironmentSatisfaction

#### Logistic Regression Model
```{pLogisticRegression eval=FALSE}
warnings.filterwarnings('ignore')

logreg = LogisticRegression()
logreg.fit(X, Y)

y_pred = logreg.predict(X_test)
print('Accuracy of logistic regression classifier on test set: {:.5f}'.format(logreg.score(X_test, Y_test)))

denom = (2.0*(1.0+np.cosh(logreg.decision_function(X))))
F_ij = np.dot((X/denom[:,None]).T,X) ## Fisher Information Matrix
Cramer_Rao = np.linalg.inv(F_ij) ## Inverse Information Matrix
sigma_estimates = np.array([np.sqrt(Cramer_Rao[i,i]) for i in range(Cramer_Rao.shape[0])]) # sigma for each coefficient
z_scores = logreg.coef_[0]/sigma_estimates # z-score for eaach model coefficient
p_values = [stat.norm.sf(abs(x))*2 for x in z_scores] ### two tailed test for p-values

print(sorted(list(zip(p_values, columns_without_attrition)))[:5])
```
**Accuracy of logistic regression classifier on test set: 0.85616**
![](screenshots/logistic.png)
In order of most important features:
1. DistanceFromHome |
2. EnvironmentSatisfaction | 
3. JobSatisfaction | 
4. NumCompaniesWorked | 
5. Overtime

#### Most Influential Factors
The Extratree classifier model (conducted in Python) and the Random Forrest model (conducted in R) both presented Overtime, Age, and MonthlyIncome as the top three most influential features.

<center><h1>Interesting Trends - In Python</h1></center>

An analysis of trends in job roles, attrition, and other factors that were commonly featured in the models above.

#### YearsSinceLastPromotion and YearsInCurrentRole

```{pyearsSinceLastPromVRole eval=FALSE}
sns.barplot(x="YearsSinceLastPromotion", y="JobRole", data=unchanged_attrition_df)
```
![](case_study_two_files/case_study_two_23_1.png)

There seems to be a significant higher average YearSinceLastPromotion for people with roles that are Managerial or Executive.

```{pyearsSinceLastPromVRoleWAttrition eval=FALSE}
sns.barplot(x="YearsSinceLastPromotion", y="JobRole",hue= "Attrition", data=unchanged_attrition_df)
```
![](case_study_two_files/case_study_two_24_1.png)

Research directors who have left their positions have an average of nearly 14 YearsSinceLastPromotion which is significantly greater than directors who are still in their position. Directorship positions often tend to be condsidered as very high position which may not result in promotions as often, it is important to consider high position roles for promotions on regular interval to more effectively maintain Research Directors.

```{pyearsInCurrentRole eval=FALSE}
sns.barplot(x="YearsInCurrentRole", y="JobRole", data=unchanged_attrition_df)
```
![](case_study_two_files/case_study_two_25_1.png)

Higher level positions tend to have greater YearsInCurrentRoles as opposed to more junior to mid-level positions.

```{pyearsInCurrentRoleWAttrition eval=FALSE}
sns.barplot(x="YearsInCurrentRole", y="JobRole", hue= "Attrition", data=unchanged_attrition_df)
```
![](case_study_two_files/case_study_two_26_1.png)

This relationship is similar to YearsSinceLastPromotion except that Managers are also considered as higher risk to leave when staying in the same role.

#### DistanceFromHome

```{pDistanceFromHome eval=FALSE}
sns.barplot(x="DistanceFromHome", y="JobRole", data=unchanged_attrition_df)
```
![](case_study_two_files/case_study_two_28_1.png)

There is no relationship between job role and distance from home.

```{pDistanceFromHomeWAttrition eval=FALSE}
sns.barplot(x="DistanceFromHome", y="JobRole", hue= "Attrition", data=unchanged_attrition_df)
```
![](case_study_two_files/case_study_two_29_1.png)

However, there is a trend that greater the distance from home the more likely someone is going to leave their job. It is evident in some positions more than others.

#### NumCompaniesWorked

```{pNumCompaniesWorked eval=FALSE}
sns.barplot(x="NumCompaniesWorked", y="JobRole", data=unchanged_attrition_df)
```
![](case_study_two_files/case_study_two_30_1.png)


```{NumCompaniesWorkedWAttrition eval=FALSE}
sns.barplot(x="NumCompaniesWorked", y="JobRole", hue="Attrition", data=unchanged_attrition_df)
```
![](case_study_two_files/case_study_two_31_1.png)

Sales executives who leave tend to work for nearly an average of two more companies that their counterparts who remain.

#### EnvironmentSatisfaction

```{pEnvironmentSatisfaction eval=FALSE}
sns.barplotsns.barplot(x="EnvironmentSatisfaction", y="JobRole", data=unchanged_attrition_df)
```
![](case_study_two_files/case_study_two_32_1.png)


```{pEnvironmentSatisfactionWAttrition eval=FALSE}
sns.barplot(x="EnvironmentSatisfaction", y="JobRole",hue= "Attrition", data=unchanged_attrition_df)
```
![](case_study_two_files/case_study_two_33_1.png)

People who leave tend to have less environment satisfaction than their counterparts who stay.

#### Overtime

```{pOvertimeAttrition eval=FALSE}
sns.countplot(y="OverTime", hue="Attrition", data=unchanged_attrition_df)
```
![](case_study_two_files/case_study_two_34_1.png)

The overall ratio of people who left their job that worked overtime was significantly larger than the ratio of people who did not leave their jobs.

```{pJobRoleOvertimeWAttrition eval=FALSE}
sns.countplot(y="JobRole", hue="OverTime", data=unchanged_attrition_df)
```
![](case_study_two_files/case_study_two_35_1.png)

The ratio of people who work overtime regardless of their roles is much higher than their attrition rate by their role alone.

#### Age

```{pAgeAttrition eval=FALSE}
sns.barplot(x="Age", y="Attrition", data=unchanged_attrition_df)
```

![](case_study_two_files/case_study_two_36_1.png)

```{pAgeJobRole eval=FALSE}
sns.barplot(x="Age", y="JobRole", data=unchanged_attrition_df)
```
![](case_study_two_files/case_study_two_37_1.png)


```{pAgeJobeRoleWAttrition eval=FALSE}
sns.barplot(x="Age", y="JobRole", hue="Attrition", data=unchanged_attrition_df)
```
![](case_study_two_files/case_study_two_38_1.png)

There is a trend that attrition is more likely for younger people however, those age trends are not as apparent based off role. Roles such as Research Directors have a higher average age for people who left their job while in Human resources the trend is the opposite.

<center><h1>Executive Summary</h1></center>

There are three reliable factors that impact attrition: Overtime, Age, and Monthly Income. Greater Overtime requirements, younger employees, and low monthly incomes contribute significantly to attrition. Other factors such as Environment Satisfaction, Number of Companies Worked For, and Job Satisfaction can also be more minor contributors to job attrition. As executives, there are a few next steps we recommend you take. First, if more data can be acquired then we can build a stronger model. Currently, we have a strong sense of the top three contributing factors but we would like to get a better sense of the top five to ten factors. Secondly, location based data would be very beneficial for providing more insight into geographical areas, which would allow for more targeted strategies. Lastly, if we can get more data then we can more confidently address some of the less influential factors and see how attrition is impacted. For example, factors such as Job Involvement and Years In Current Role can be easier fixes than increasing Monthly Income. If we can build a stronger model that can expose factors which can be "easier wins" then those can be prioritized first.